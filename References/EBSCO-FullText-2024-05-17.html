<!DOCTYPE html><html><body><dl><dt><b>Title: </b></dt><dd>Deep learning.</dd><dt><b>Authors: </b></dt><dd>Dunn, Tamara</dd><dt><b>Source: </b></dt><dd>Salem Press Encyclopedia of Science, 2023. 3p. </dd><dt><b>Document Type: </b></dt><dd>Article</dd><dt><b>Subject Terms: </b></dt><dd><a href="/results?q=DE+%22Deep+learning%22" data-type="search" data-query="DE+%22Deep+learning%22">Deep learning</a></dd> <dd><a href="/results?q=DE+%22Artificial+intelligence%22" data-type="search" data-query="DE+%22Artificial+intelligence%22">Artificial intelligence</a></dd><dt><b>Abstract: </b></dt><dd>Deep learning consists of artificial neurons in multilayered networks that through algorithms can teach software to train itself to recognize objects such as language and images. It is a subset of machine learning—the ability of a machine to &quot;learn&quot; with experience instead of programming—and is a part of artificial intelligence. Patterned after brain science, deep learning involves feeding a neural network with large amounts of data to train the machine for classification. The machine is given an object to identify and will process it through several network layers. As the process continues, the machine goes from simple layers to ones that are more complicated until an answer is reached. Algorithms instruct the neurons how to respond to improve the results.</dd><dt><b>Full Text Word Count: </b></dt><dd>1307</dd><dt><b>Accession Number: </b></dt><dd>119214353</dd></dl><article> <h1>Deep learning</h1> <p> <em> Deep learning </em> consists of artificial neurons in multilayered
 networks that through <a id="eplink_E54B1DD1AE787324FDE7CE3290D84F31" href="/html/89316871/ers">algorithms</a> can teach software to train
 itself to recognize objects such as language and images. It is a subset of
 <a id="eplink_5A7D087C57F4CCC3A65936BC4D6E0E34" href="/html/90558380/ers">machine
 learning</a> —the ability of a machine to "learn" with experience
 instead of programming—and is a part of artificial intelligence. Patterned after
 brain science, deep learning involves feeding a neural network with large amounts
 of data to train the machine for classification. The machine is given an object to
 identify and will process it through several network layers. As the process
 continues, the machine goes from simple layers to ones that are more complicated
 until an answer is reached. Algorithms instruct the neurons how to respond to
 improve the results.</p> <figure align="left" > <img src="https://imagesrvr.epnet.com/embimages/ers/sp/embedded/rssalemscience-20160829-52-144038.jpg" alt="Artificial neural network with layer coloring" title="Artificial neural network with layer coloring By Glosser.ca [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons" data-protect="true"/> <figcaption>Artificial neural network with layer coloring By Glosser.ca [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons</figcaption> </figure> <figure align="left" > <img src="https://imagesrvr.epnet.com/embimages/ers/sp/embedded/rssalemscience-20160829-52-144039.jpg" alt="The three interlocking gears in the Entangled Learning model: practice-based learning, social learning, and design for deep learning." title="The three interlocking gears in the Entangled Learning model: practice-based learning, social learning, and design for deep learning. By Lwhisler (Own work) [CC BY-SA 4.0 (http://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons" data-protect="true"/> <figcaption>The three interlocking gears in the Entangled Learning model: practice-based learning, social learning, and design for deep learning. By Lwhisler (Own work) [CC BY-SA 4.0 (http://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons</figcaption> </figure> <p> <strong>Brief History</strong> </p> <p>Neuron networks were first introduced in the 1950s as biologists were mapping
 out the workings of the human <a id="eplink_3AF8F8FF0F6C08548DD0971A19EBCBF1" href="/html/87690303/ers">brain</a> . Computer scientists were looking
 beyond logical applications to replicate thinking in machines. In 1958, research
 psychologist Frank Rosenblatt applied these theories to design the perceptron, a
 single-layered network of simulated neurons using a room-sized computer. Through
 their connections, the neurons would relay a value, or "weight," of either 1 or 0
 to correspond with a shape. However, after several tries, the machine would not
 recognize the right shape. Rosenblatt applied supervised learning, training the
 perceptron to output the correct answer with the machine developing an algorithm
 to tweak the weights to get the correct answer.</p> <p>Rosenblatt's algorithm, however, did not apply to multilayered networks,
 limiting the perceptron's ability to perform more complex tasks. In a 1969 book,
 artificial-intelligence scientists Marvin Minsky and <a id="eplink_94F49C54DE3809B3203516FA89771D09" href="/html/89876682/ers">Seymour
 Papert</a> believed that making more layers would not make
 perceptrons more useful. Neural networks were abandoned for nearly two
 decades.</p> <p>In the mid-1980s, researchers Geoffrey Hinton and Yann LeCun revived interest in neuron networks, with the belief that a brain-like structure was needed to fulfill the potential of artificial intelligence. Instead of only outputting an answer, the goal was to create a multilayered network that would allow the machine to learn from past mistakes. The duo and other researchers used a learning algorithm called backpropagation that would allow data to pass through multiple layers and the network to make adjustments to give the right answer. This spawned technology in the 1990s that could read handwritten text. However, like perceptrons, backpropagation had its limitations and required much data to be fed into a machine. Other researchers were developing alternative learning algorithms that did not require neurons. The networks fell out of fashion again.</p> <p>Hinton returned to neuron research in the mid-2000s, and in 2006, he developed methods to teach larger networks with multiple layers of neurons. This required a progressive set of layers, as each layer recognized a certain feature and moved on to the next until the system identified the object. In 2012, Hinton and two students won an image recognition contest with their software, identifying one thousand objects. Focus returned to neuron networks and deep learning was pushed to the forefront.</p> <p> <strong>Overview</strong> </p> <p>Since 2011, deep learning has created advancements in <a id="eplink_15C30074E682B463A53F794FFCA6EA55" href="/html/89250362/ers">artificial
 intelligence</a> . Technology giants such as <a id="eplink_DC03387AC6FACBA78872D75D2DC2AB76" href="/html/89138950/ers">Google</a> ,
 Facebook, Microsoft, and Chinese web services company Baidu are using
 deep-learning applications to access massive amounts of data. Microsoft and Google
 employ it to power their speech- and image-recognition products, including their
 voice-activated searches, translation tools, and photo searches. Google Brain,
 with Hinton on its research team, has more than one thousand deep-learning
 projects under development, and Google-acquired DeepMind has combined deep
 learning with other technological approaches, such as deep-reinforcement learning,
 to solve issues such as energy efficiency and gaming. Facebook uses deep learning
 in its face-recognition software whenever a photo is uploaded to the service to
 identify or tag people. Through Facebook Artificial Intelligence Researchers
 (FAIR), started by LeCun, the company is exploring ways to improve neuron
 networks' ability to understand language.</p> <p>Hardware has evolved to keep up with the demand for deep learning. With neural networks requiring large amounts of data, companies want computer chips that can deliver fast results. This has benefited firms such as Nvidia and Advanced Micro Devices that develop computer chips that render rich images for video game players. These chips, known as graphics processing units (GPUs), can compute hundreds more deep-learning processes compared to traditional computer processing units (CPUs).</p> <p>Google has developed its own chips called tensor processing units, and IBM is making brain-like chips called TrueNorth. Start-up companies are designing chips specifically for deep learning, with some catching the attention of chip giants such as Intel, known for producing CPUs.</p> <p>One of deep learning's goals is linked to artificial intelligence's early aspirations—mastering natural language. While companies have created interactive, speech-recognition applications such as Apple's Siri and Amazon's Alexa through deep learning, natural conversation has yet to be achieved. Some researchers believe that neuron networks operate in a manner that is too simple for conversations to occur. Critics of deep learning, such as New York University professor Gary Marcus and Massachusetts Institute of Technology professor Josh Tenenbaum, say that while immense amounts of data are required for a machine to learn something, humans need only a small sum of information and a collection of basic skills. Nevertheless, Facebook and Google continue to back research into developing better language capabilities.</p> <p>Deep learning is also being applied to other fields. For example, it is instrumental in the development of self-driving cars. The vehicle's neuron networks process decisions based on a human driver's likely responses and making adjustments to improve its driving. Google and Baidu are backing research in this field. Deep-learning programs may be used in medicine. Start-ups are using improved image recognition to analyze medical imagery and foster drug development.</p> <p>The push for deep learning has created a demand for computer scientists, with companies such as Microsoft, Amazon, and Google hiring the top researchers in the field. These firms have also made open source software available to developers interested in exploring the concept further.</p> <p> <strong>Bibliography</strong> </p> <p>Bengio, Yoshua. "Machines Who Learn." <em> Scientific American </em> , vol. 314, no. 6, 2016, pp. 46–51.</p> <p>Burns, Ed, and Kate Brush. "Deep Learning." <em> Tech Target </em> , 2022, www.techtarget.com/searchenterpriseai/definition/deep-learning-deep-neural-network. Accessed 30 Dec. 2022.</p> <p>Clark, Don. "Computer Chips Evolve to Keep Up with Deep Learning." <em> Wall Street Journal </em> , 11 Jan. 2017, www.wsj.com/articles/computer-chips-evolve-to-keep-up-with-deep-learning-1484161286. Accessed 22 Dec. 2022.</p> <p>Hof, Robert D. "Deep Learning." <em> MIT Technology Review </em> , www.technologyreview.com/s/513696/deep-learning. Accessed 16 Jan. 2017.</p> <p>Knight, Will. "AI's Unspoken Problem." <em> MIT Technology Review </em> , vol. 119, no .5, 2016, pp. 28–37.</p> <p>---. "Kindergarten for Computers." <em> MIT Technology Review </em> , vol. 119, no. 1, 2016, pp. 52–58.</p> <p>Metz, Cade. "2016: The Year That Deep Learning Took Over the Internet." <em> Wired </em> , 25 Dec. 2016, www.wired.com/2016/12/2016-year-deep-learning-took-internet. Accessed 16 Jan. 2017.</p> <p>---. "Finally, Neural Networks That Actually Work." <em> Wired </em> , 21 Apr. 2015, www.wired.com/2015/04/jeff-dean. Accessed 16 Jan. 2017.</p> <p>Parloff, Roger. "Why Deep Learning Is Suddenly Changing Your Life." <em> Fortune </em> , 28 Sept. 2016, fortune.com/ai-artificial-intelligence-deep-machine-learning. Accessed 16 Jan. 2017.</p> <p>Simonite, Tom. "Teaching Machines to Understand Us." <em> MIT Technology Review </em> , 6 Aug. 2015, www.technologyreview.com/s/540001/teaching-machines-to-understand-us/. Accessed 16 Jan. 2017.</p> </article></body><footer><small>&copy;2024 EBSCO Industries, Inc. All rights reserved<br/>EBSCO | 10 Estes Street | Ipswich, MA 01938</small></footer></html>